{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "\n",
    "# import febrisk\n",
    "lib_path = os.path.join(os.path.abspath('.'), '..\\\\..')\n",
    "sys.path.append(lib_path)\n",
    "from febrisk.performance import cal_return, update_weights, cal_return_attribution, cal_risk_attribution\n",
    "from febrisk.risk import expected_shortfall\n",
    "from febrisk.simulation import CopulaSimulator\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "Part 1 – Calculate the maximum SR portfolio using the method from last week’s homework for the following stocks: AAPL, MSFT, BRK-B, CSCO, and JNJ.  \n",
    "Use the returns from the end of the history (1-14) until the end of February.  \n",
    "Calculate the Ex-Post Return Attribution for each Stock.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_3 = pd.read_csv('F-F_Research_Data_Factors_daily.csv', parse_dates=['Date']).set_index('Date')\n",
    "ff_4th = pd.read_csv('F-F_Momentum_Factor_daily.csv', parse_dates=['Date']).set_index('Date')\n",
    "ff_4 = ff_3.join(ff_4th, how='right') / 100  # turn pct values into actual numbers\n",
    "all_rets = pd.read_csv('DailyReturn.csv', parse_dates=['Date']).set_index('Date')\n",
    "stocks = ['AAPL', 'MSFT' ,'BRK-B', 'CSCO', 'JNJ']\n",
    "factors = ['Mkt-RF', 'SMB', 'HML', 'Mom']\n",
    "reg_dataset = all_rets[stocks].join(ff_4)  # dataset for regression\n",
    "\n",
    "\n",
    "# 1. calculate arithmetic E(r) in past 10 years\n",
    "avg_factor_rets = ff_4.loc['2012-1-14':'2022-1-14'].mean(axis=0)\n",
    "avg_daily_rets = pd.Series(dtype='float64')\n",
    "betas = np.empty((len(stocks), len(factors)))\n",
    "for i, stock in enumerate(stocks):\n",
    "    # calculate betas\n",
    "    # r_stock - r_f = alpha + sum(beta_i * factor_i) + error\n",
    "    model = sm.OLS(reg_dataset[stock] - reg_dataset['RF'], sm.add_constant(reg_dataset[factors]))\n",
    "    result = model.fit()\n",
    "    betas[i] = result.params[1:]\n",
    "\n",
    "    # E(r_stock) = alpha + sum(beta_i * E(factor_i)) + E(r_f)\n",
    "    # assume alpha = 0\n",
    "    avg_daily_rets[stock] = (result.params[factors] * avg_factor_rets[factors]).sum() \\\n",
    "                            + avg_factor_rets['RF'] \n",
    "    \n",
    "\n",
    "# 2. geometric annual returns: mean and covariance\n",
    "geo_means = np.log(1 + avg_daily_rets)*255  \n",
    "geo_covariance = np.log(1 + all_rets[stocks]).cov()*255     \n",
    "\n",
    "\n",
    "init_weights = np.array([0.10075988181538116, 0.20950981862533438,\n",
    "                         0.43839111238558587, 0.08118475735284336,\n",
    "                         0.17015442982085532])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_prices = pd.read_csv(\"updated_prices.csv\", parse_dates=['Date'])\n",
    "updated_rets = cal_return(updated_prices.set_index('Date')[stocks], method='arithmetic')\n",
    "updated_weights = update_weights(init_weights, updated_rets.values)\n",
    "weighted_rets = updated_weights * updated_rets.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00458892, -0.0070828 , -0.00369351, -0.00754389, -0.00215082])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_return_attribution(weighted_rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00156469, 0.00315964, 0.00469129, 0.00089426, 0.00140861])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_risk_attribution(weighted_rets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2  \n",
    "Using the same data as Problem 1, attribute realized risk and return to the Fama French 3+Momentum model. Report the residual total as Portfolio Alpha.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_ff_3 = pd.read_csv('updated_F-F_Research_Data_Factors_daily.csv', parse_dates=['Date']).set_index('Date')\n",
    "updated_ff_4th = pd.read_csv('updated_F-F_Momentum_Factor_daily.csv', parse_dates=['Date']).set_index('Date')\n",
    "updated_ff_4 = updated_ff_3.join(updated_ff_4th, how='right') / 100  # turn pct values into actual numbers\n",
    "updated_factor_rets = pd.concat([ff_4, updated_ff_4]).loc[updated_rets.index, factors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_factor_weights = updated_weights @ betas\n",
    "weighted_factor_rets = updated_factor_weights * updated_factor_rets.values\n",
    "resid_rets = weighted_rets.sum(axis=1) - weighted_factor_rets.sum(axis=1)\n",
    "weighted_factor_rets = np.hstack((weighted_factor_rets, resid_rets[:, np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0445635 ,  0.00010344,  0.00242659, -0.00060942,  0.01758294])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_return_attribution(weighted_factor_rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00977524, -0.00010896, -0.00071545,  0.00012717,  0.00264049])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_risk_attribution(weighted_factor_rets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "Using the same data as Problem 1 and assuming a 0 mean return, fit a t distribution to each stock return series. Simulate the system using a Gaussian Copula. Find the Risk Parity portfolio using ES as the risk measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15420278, 0.14256632, 0.26515987, 0.15078634, 0.28728468])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cal_pfl_volatility(weights, covariance):\n",
    "    return np.sqrt(weights @ covariance @ weights.T)\n",
    "\n",
    "\n",
    "def cal_component_std(weights, covariance):\n",
    "    pfl_vol = cal_pfl_volatility(weights, covariance)\n",
    "    return weights * (covariance @ weights.T) / pfl_vol\n",
    "\n",
    "\n",
    "def cal_component_std_sse(weights, covariance, budget=None):\n",
    "    if not budget:\n",
    "        budget = np.ones_like(weights)\n",
    "\n",
    "    component_std = cal_component_std(weights, covariance) / budget\n",
    "    dev = component_std - np.mean(component_std)\n",
    "    return dev @ dev.T\n",
    "\n",
    "\n",
    "def gen_risk_parity_portfolio_on_std(covariance, budget=None):\n",
    "    nassets = covariance.shape[0]\n",
    "    \n",
    "    x0 = np.array([1 / nassets for _ in range(nassets)])\n",
    "    constraints = {'type':'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "    bounds = ((0, 1) for _ in range(nassets))\n",
    "    results = scipy.optimize.minimize(lambda w: 1e5*cal_component_std_sse(w, covariance, budget), \n",
    "                                      x0=x0, bounds=bounds, constraints=constraints)\n",
    "    return results.x\n",
    "\n",
    "std_rp_weights = gen_risk_parity_portfolio_on_std(geo_covariance.values)\n",
    "std_rp_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16375984, 0.12566433, 0.29775739, 0.1390347 , 0.27378374])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cal_pfl_es(weights, returns):\n",
    "    return expected_shortfall(returns @ weights.T)\n",
    "\n",
    "\n",
    "def cal_component_es(weights, returns, delta=1e-6):\n",
    "    nassets = len(weights)\n",
    "    es = cal_pfl_es(weights, returns)\n",
    "    component_es = np.empty(nassets, dtype=float)\n",
    "\n",
    "    for i in range(nassets):\n",
    "        weight_i = weights[i]\n",
    "        weights[i] += delta\n",
    "        component_es[i] = weight_i * (cal_pfl_es(weights, returns) - es) / delta\n",
    "        weights[i] -= delta\n",
    "    \n",
    "    return component_es\n",
    "\n",
    "\n",
    "def cal_component_es_sse(weights, returns, budget=None, delta=1e-6):\n",
    "    if not budget:\n",
    "        budget = np.ones_like(weights)\n",
    "    component_es = cal_component_es(weights, returns, delta) / budget\n",
    "    dev = component_es - np.mean(component_es)\n",
    "    return dev @ dev.T\n",
    "\n",
    "\n",
    "def gen_risk_parity_portfolio_on_es(returns, budget=None):\n",
    "    nassets = returns.shape[1]\n",
    "    x0 = np.array([1 / nassets for _ in range(nassets)])\n",
    "    constraints = {'type':'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "    bounds = ((0, 1) for _ in range(nassets))\n",
    "    results = scipy.optimize.minimize(lambda w: 1e5*cal_component_es_sse(w, returns, budget), \n",
    "                                      x0=x0, bounds=bounds, constraints=constraints)\n",
    "    return results.x\n",
    "\n",
    "\n",
    "dists = []\n",
    "for stock in stocks:\n",
    "    df, loc, scale = scipy.stats.t.fit(all_rets[stock])\n",
    "    dists.append(scipy.stats.t(df=df, loc=loc, scale=scale))\n",
    "    \n",
    "copula = CopulaSimulator(all_rets[stocks].values, dists)\n",
    "sim_rets = copula.simulate(5000)\n",
    "gen_risk_parity_portfolio_on_es(sim_rets)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2a7c3d20469dda411fd3211f02092a920078d1ca97c72d3dc7928b841d3a44a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
